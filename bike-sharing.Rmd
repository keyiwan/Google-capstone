---
title: "Google Data Analytics Certificate - Bike Share Case Study - Part1"
author: "K.Wan"
date: "2022/3/28"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale("LC_ALL",  "English") ## language set
```

## Case Background  

This analysis is for case study 1 from the Google Data Analytics Certificate (Cyclistic). It's originally based on the case study *[Sophisticated, Clear, and Polished: Divvy and Data Visualization](https://artscience.blog/home/divvy-dataviz-case-study)* by Kevin Hartman.  


I will be using the [Divvy dataset](https://divvy-tripdata.s3.amazonaws.com/index.html) for this capstone project. The purpose of this script is to consolidate downloaded Divvy data into a single dataframe and then conduct simple analysis to help answer the key question:"**In what ways do members and casual riders use Divvy bikes differently?**"  

In the Google Data Analytics Certificate programme, there are mainly six phases: **Ask**, **Prepare**, **Process**, **Analyse**, **Share**, and **Act**. Since this capstone challenge has outlined my tasks as a junior data analyst in this fictitious bike sharing company and provided real first-party data, my job will jump right into the **Process** of data. 

## Process

### Get data ready

For convenience, I will first load three useful packages: `tidyverse` for data import and wrangling, `lubridate` for date functions, and `ggplot2` for visualisation.

```{r loading packages, warning=FALSE}
# install.packages("tidyverse")
# install.packages("lubridate")

library(tidyverse) # data import and tidy
library(lubridate) # functions set for date, time
library(ggplot2) # helps visualise data
```

**1. Upload datasets**  

In this analysis, I will investigate data in the past twelve months, spanning from December 2021 to February 2022.

Function `readr::read_csv` is preferred to `read.csv` of package `base` for a number of reasons: <https://r4ds.had.co.nz/data-import.html#compared-to-base-r>.

```{r import data, message=FALSE, results='hide'}
dt_2202 <- read_csv("202202-divvy-tripdata.csv", na = "")
# na = "" is to assign blank cell with NA
dt_2201 <- read_csv("202201-divvy-tripdata.csv", na = "")
dt_2103 <- read_csv("202103-divvy-tripdata.csv", na = "")
dt_2104 <- read_csv("202104-divvy-tripdata.csv", na = "")
dt_2105 <- read_csv("202105-divvy-tripdata.csv", na = "")
dt_2106 <- read_csv("202106-divvy-tripdata.csv", na = "")
dt_2107 <- read_csv("202107-divvy-tripdata.csv", na = "")
dt_2108 <- read_csv("202108-divvy-tripdata.csv", na = "")
dt_2109 <- read_csv("202109-divvy-tripdata.csv", na = "")
dt_2110 <- read_csv("202110-divvy-tripdata.csv", na = "")
dt_2111 <- read_csv("202111-divvy-tripdata.csv", na = "")
dt_2112 <- read_csv("202112-divvy-tripdata.csv", na = "")
```

**2. Merging multiple tables**  

Before combining tables into a single data frame, I need to observe components of each table, specifically column names of each file, in order to make merge compatible.

Following code chunk displays some useful techniques when dealing with this issue.

```{r merge method, eval=FALSE}
# check column names, for they need to match perfectly before we can combine files by rows
colnames(dt_2103)
colnames(dt_2104)
# if there are mismatched column names, we can rename columns to make them consisten, e.g.:
dt_2103 <- rename(dt_2103, new_name = old_name)
# inspect the dataframes and look for inconguencies
str(dt_2103)
# change column types if needed, e.g. convert ride_id to character so that they can stack correctly
dt_2103 <- mutate(dt_2103, ride_id = as.character(ride_id))
```

```{r merge}
dt_all <- rbind(dt_2103, dt_2104, dt_2105, dt_2106,
                dt_2107, dt_2108, dt_2109, dt_2110,
                dt_2111, dt_2112, dt_2201, dt_2202)
# dplyr::bind_rows() is a good alternative 
glimpse(dt_all)  # view the structure of the dataframe
old_records <- nrow(dt_all)
```

The resulting tibble consists of `r format(nrow(dt_all), big.mark = ",", scientific = FALSE)` rows with `r ncol(dt_all)` columns of character and numeric data such as ride id, start and end time of ride, start location, geographic coordinates, etc.


### Data cleaning

Data tidying usually involves checking for missing values, trailing white spaces, duplicate records, and other data anomalies.

**1. Duplicates**  

To check duplicates, `distinct()` is used to check duplicates based on all of columns while it is also an option to screen on `ride_id` alone. The result shows no duplicates removed.

```{r duplicates}
dt_all <- dt_all %>% 
  distinct()
# distinct(ride_id, .keep_all = TRUE), based on ride_id alone
## no duplicates removed
nrow_old <- nrow(dt_all)
```

**2. Missing values**  

```{r missing total, eval=FALSE, echo=FALSE}
apply(dt_all, 2, function (x) {sum(is.na(x))})
```

```{r missing proportion}
apply(dt_all, 2, function (x) {mean(is.na(x))})
```

Since there is no missing value on identifier column `ride_id` and the proportion of missing data on each variable is no more than 14%, it is practically acceptable to drop rows containing `NA`.

```{r drop_na}
dt_all <- dt_all %>% drop_na(start_station_name, start_station_id, end_station_name, end_station_id,
                       end_lat, end_lng)
nrow_new <- nrow(dt_all)
```

Then `r format(nrow_old - nrow_new, big.mark = ",", scientific = FALSE)` rows have been deleted, accounting for merely `r paste0((round((nrow_old-nrow_new)/nrow_old, digits=2))*100, "%")` of original data size.

**3. Strings**  

When it comes to characters, typos or miscellaneous errors are not uncommon. Error-prone strings can be due to careless data entry or even on purpose, they are not easy to spot and even more time-consuming or painstaking to fix.

Note that there are a few station ids representing more than one station name due to either typo or unknown minor changes. The following steps show how to identify mistyped station names and then conduct error checking.

However, manually fixing the problem of station names is not essential to answering questions of our interest and using station id alone is sufficient. 

```{r, eval=FALSE}
dup_id <- dt_all %>% 
  count(start_station_id, start_station_name) %>% 
  filter(duplicated(start_station_id)) %>% 
  select(start_station_id) # return ids appearing more than once

dup_id <- unlist(as.list(dup_id)) # transform tibble into list first then to vector as `as.vector()` not applicable 

dt_all %>% 
  count(start_station_id, start_station_name) %>% 
  filter(start_station_id %in% dup_id)
```

**4. Dates**  

Addressing `date` columns is necessary for analysing the dynamics of rides on different time frames. With each ride record contains the exact time points for start and end, this analysis can come up with a more meaningful metric - ride length - a calculated field using the difference between columns `ended_at` and `started_at`.

```{r date_formatting}
dt_all <- dt_all %>% 
  mutate(started_at = ymd_hms(started_at),
         ended_at = ymd_hms(ended_at),
         ride_length = ended_at - started_at) # figures in seconds, difftime is an alternative function
```

```{r drill_date, eval=FALSE, echo=FALSE}
dt_all %>% 
  mutate(ride_length = ended_at - started_at,
         start_day = mday(started_at),
         end_day = mday(ended_at),
         past_day = end_day - start_day) %>% 
  filter(past_day != 0) %>%  ## not on the same day, difftime as seconds, which accounts for day differences
  head(10)
```

Convert the unit of ride length from `seconds` to `minutes`.
```{r minutes}
dt_all <- dt_all %>% 
  mutate(ride_length_m = round((as.numeric(ride_length)/60), digits = 2))
```

Dissect `date` by year, month, day and weekday.  

The reason is that data can only be aggregated at the ride-level, which is too granular based on original date formats. So I will add some additional columns of data -- such as day, month, year, weekday -- that provide additional opportunities to aggregate the data.

```{r dissect date}
dt_all <- dt_all %>% 
  mutate(start_year = year(started_at), 
         end_year = year(ended_at), 
         start_month = month(started_at), # month as number
         end_month = month(ended_at),
         start_Month = month(started_at, label = TRUE), # months as Jan, Feb 
         end_Month = month(ended_at, label = TRUE), 
         start_day = mday(started_at), # day of month
         end_day = mday(ended_at), 
         start_wday = wday(started_at, label = TRUE), # day of week
         end_wday = wday(ended_at, label = TRUE))
```

**5. Remove "bad" data**  

Despite no clear definition of "bad" data, here I need to spot data that takes only a small number while having values that can disproportionately influence our findings ,and then sometimes delete them.

In this case, I will remove extremely large and negative values in `ride_length` since they make no practical sense for the majority of users and can distort data distribution, but how to address outliers is a case-by-case study.

Specifically, records for trips less than 60 seconds (false starts) or longer than 24 hours were removed. Bikes out longer than 24 hours are considered stolen and the rider is charged for a replacement. Idea inspired from: <https://vibrantoutlook.wordpress.com/2021/09/19/google-data-analytics-certificate-capstone-project/>

```{r filter, tidy=TRUE}
dt_all02 <- dt_all %>% 
  filter(end_day - start_day < 2 & 
           between(ride_length_m, 1, 1440))
# filter rides spanning across within 2 days 
# while not lasting above 24 hours/ 1440 minutes, 
# above 1 for filtering out negative results and false starts
```

`r format(nrow_new - nrow(dt_all02), big.mark=",", scientific=FALSE)` records removed.

Meanwhile, rides that started or ended at DIVVY CASSETTE *REPAIR* MOBILE STATION or HUBBARD ST BIKE *CHECKING* (LBS-WH-TEST)  or WATSON *TESTING* DIVVY could be removed as these are administrative stations.  
(Link for reference:<https://vibrantoutlook.wordpress.com/2021/09/19/google-data-analytics-certificate-capstone-project/>)  

For brevity, I use `str_detect()` to find matches containing any characters of `REPAIR`, `TESTING` or `CHECKING`.

```{r testing, eval=FALSE, echo=FALSE}
## this code chunk is used for experimenting
station_name <- dt_all02 %>% 
  count(start_station_name)
station_name %>% 
  filter(str_detect(start_station_name, "REPAIR|TESTING|CHECKING")) 
# 4 records returned

ending_station_name <- dt_all02 %>% 
  count(end_station_name)

ending_station_name %>% 
  filter(str_detect(end_station_name, "REPAIR|TESTING|CHECKING")) # 4 records returned

station_name %>% 
  filter(start_station_name == "WATSON TESTING DIVVY") # double check for explicit search
```

```{r repair}
# find matches
dt_all02 %>% 
  filter(str_detect(start_station_name, "REPAIR|TESTING|CHECKING") | str_detect(end_station_name, "REPAIR|TESTING|CHECKING")) %>% 
  select(ride_id, start_station_id, end_station_id)
```

8 records detected to be removed.

```{r nonrepair}
# remove matches
dt_all02 <- dt_all02 %>% 
  filter(!str_detect(start_station_name, "REPAIR|TESTING|CHECKING") & !str_detect(end_station_name, "REPAIR|TESTING|CHECKING")) # !(A | B) == !A & !B
```

At the end of data cleaning, `r format(old_records-nrow(dt_all02), big.mark = ",", scientific = FALSE)` rows have been deleted, accounting for `r paste0((round((old_records-nrow(dt_all02))/old_records, digits=2))*100, "%")` of original data size.

Alternatively, I decide to export the cleaned data as a new csv file and document the following process of analysing and visualisation directly on the new file.

```{r data_export}
write_csv(dt_all02, file = "cleaned_data.csv")
```


## Analyse

Here comes the first question: How many observations fall under each user type?

```{r category check}
(dt_all_freq <- dt_all02 %>% 
  count(member_casual))
```
As shown above, more than `r format(dt_all_freq[2,2]-dt_all_freq[1,2], big.mark = ",", scientific = FALSE)` rides were by members than casual riders in the whole time period. 

Then, I conduct descriptive analysis on **ride length**, answering questions like:  

> Is there a difference on total/average ride lengths among the two user groups?   
> How is that difference varied on monthly/daily level? 


**1. Total ride lengths**   

Let's have a look at the total, average, median, minimum and maximum. 

```{r total length} 
 dt_all02 %>%  
   group_by(member_casual) %>%  
   summarise(number_of_rides = n(), 
             total_length = sum(ride_length_m),  
             mean_length = mean(ride_length_m), 
             median_length = median(ride_length_m),  
             max_length = max(ride_length_m),  
             min_length = min(ride_length_m)) 
``` 

Overall, more rides were taken by members while casual riders enjoyed longer trips, contributing nearly double the total ride length of members' during the same time period.   

The average duration of each ride by casual riders is around `r dt_all02 %>% filter(member_casual=='casual') %>% summarise(mean_length_casual=mean(ride_length_m)) %>% round(digits=1)` minutes, two times more of the `r dt_all02 %>% filter(member_casual=='member') %>% summarise(mean_length_member=mean(ride_length_m)) %>% round(digits=1)` minutes for annual members. 


**2. Ride length by month** 

```{r month} 
 dt_all02 %>%  
   group_by(member_casual, start_Month) %>%  
   summarise(sum_length_m = sum(ride_length_m),  
             mean_length_m = mean(ride_length_m)) %>% 
   arrange(member_casual, start_Month) %>%  
   head(12) 
``` 

 It seems like the second quarter (April, May & June) to be a period that has experienced relatively longer duration of rides by casual riders. 


**3. Which day of a week induced most traffic** 

See the number of trips and average ride duration on each day of a week for all users. 

```{r weekday} 
 dt_all02 %>%  
   group_by(start_wday) %>%  
   summarise(number_of_rides = n(),  
             total_length = sum(ride_length_m),  
             mean_length = mean(ride_length_m)) 
``` 

Compare ridership, average ride length by weekday for members vs casual riders. 

```{r weekday_type} 
 dt_all02 %>%  
   group_by(member_casual, start_wday) %>%  
   summarise(number_of_rides = n(),  
             total_length = sum(ride_length_m),  
             mean_length = mean(ride_length_m)) %>%  
   arrange(member_casual, start_wday)  
``` 

Regardless of the user type, weekends undoubtedly enjoyed peak usage, which can possibly guide us in deciding marketing times.   


**4. Top 10 busiest stations** 

To better market membership of Divvy, hence attracting more casual riders to subscribe as members, stations mostly visited by casual users may be a great choice as locations for the upcoming campaign. 

```{r hotspot01} 
dt_all02 %>%  
   filter(member_casual == "casual") %>%  
   group_by(start_station_id) %>%  
   summarise(n = n()) %>%  
   arrange(desc(n)) %>%  
   select(start_station_id, n) %>%  
   head(10) 
``` 

Is there any difference on station preference between casual riders and annual members?

```{r hotspot02}
dt_all02 %>% 
  filter(member_casual == "member") %>% 
  group_by(start_station_id) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n)) %>% 
  select(start_station_id, n) %>% 
  head(10)
```

Overall, which stations can reach out to most users regardless of user type.

```{r hotspot03}
dt_all02 %>% 
  group_by(start_station_id) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n)) %>% 
  select(start_station_id, n) %>% 
  head(10)
```

As shown above, there is undeniably a distinct difference on stations choices made by members and casual riders.

Even though I think sticking to station id is easier, but for map viz, names of locations are better for presentation. Since I will focus on mainly the top 10 stations most visited by casual riders for past 12 months, only two ids in the top 10 list that also appears in the duplicated list before: `13300` and `LF-005`. So I will fix the names of stations belonging to those two ids.

It turns out, recently [Chicago's iconic Lake Shore Drive has been renamed in honor of Jean Baptiste Point DuSable](https://blockclubchicago.org/2021/10/21/lake-shore-drive-signs-now-have-its-new-name-dusable-lake-shore-drive-honoring-citys-black-founder/), so the `Lake Shore Dr` in the data set has been renamed `DuSable Lake Shore Dr` shown in more recent records. And luckily, both cases of duplicates are due to the same reason. Then I will just update the station names accordingly.

```{r renaming1}
dt_all02 %>% 
  count(start_station_id, start_station_name) %>% 
  filter(start_station_id %in% c("13300", "LF-005"))
```

```{r renaming2}
dt_all02 <- dt_all02 %>% 
  mutate(start_station_name = case_when(
    start_station_id == "13300" ~ "DuSable Lake Shore Dr & Monroe St",
    start_station_id == "LF-005" ~ "DuSable Lake Shore Dr & North Blvd",
    TRUE ~ start_station_name
  ))
```

Now, test with updated station names.

```{r test_name1}
dt_all02 %>% 
  filter(member_casual == "casual") %>% 
  group_by(start_station_id) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n)) %>% 
  select(start_station_id, n) %>% 
  head(10)
```

```{r test_name2}
dt_all02 %>% 
  filter(member_casual == "casual") %>% 
  group_by(start_station_name) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n)) %>% 
  select(start_station_name, n) %>% 
  head(10)
```


**5. The proportion of casual riders and annual members for each station**

I want to produce a table with each row representing a unique station id, columns involving number of rides by members, by casual riders, and their individual proportion of total number of rides.

```{r proportion}
(prop_dt <- dt_all02 %>% 
  group_by(start_station_id, member_casual) %>% 
  summarise(n = n()))
```
This tibble having multiple values stacked in a small number of columns needs the function `pivot_wider` to make it wider.
```{r pivot_wider}
prop_dt %>% 
  pivot_wider(names_from = member_casual, values_from = n) %>% 
  mutate(total = casual + member, casual_freq = casual/total, member_freq = member/total) %>% 
  arrange(desc(casual))
```
In result, the list of 10 stations boasting far more visits from casual riders than members is the same as before.                    

## Share  

It is time to visualise our findings to vividly demonstrate differences on riding patterns between different rider types.  

**1. Members ride more often than non-members**  

Look at the pie chart below, showing the discrepancy on numbers of rides by rider type.


```{r pie_trips1}
library(scales)
dt_all02 %>% 
  group_by(member_casual) %>% 
  count() %>% 
  ungroup() %>% 
  ggplot(aes(x = "", y = n, fill = member_casual))+
  geom_col()+
  geom_text(aes(label = member_casual), position = position_stack(vjust = 0.5)) +
  labs(title = "Number of Trips by Rider Type")+
  guides(fill = guide_legend(title = "Rider Type"))+
  coord_polar("y")
```

```{r pie_trips2}
dt_all02 %>% 
  group_by(member_casual) %>% 
  count() %>% 
  ungroup() %>% 
  mutate(perc = n/sum(n)) %>% 
  mutate(labels = scales::percent(perc)) %>% 
  ggplot(aes(x = "", y = perc, fill = member_casual))+
  geom_col()+
  geom_text(aes(label = labels), position = position_stack(vjust = 0.5)) +
  labs(title = "Number of Trips by Rider Type")+
  guides(fill = guide_legend(title = "Rider Type"))+
  coord_polar("y")
```



```{r rides_bar, echo=FALSE}
dt_all02 %>% 
  group_by(member_casual, start_wday) %>% 
  summarise(number_of_rides = n(), total_length = sum(ride_length_m), mean_length = mean(ride_length_m)) %>% 
  arrange(member_casual, start_wday) %>% 
  ggplot(aes(x = start_wday, y = number_of_rides, 
             fill = member_casual)) + 
  geom_col(position = "dodge")
```

```{r length_bar, echo=FALSE}
dt_all02 %>% 
  group_by(member_casual, start_wday) %>% 
  summarise(number_of_rides = n(), total_length = sum(ride_length_m), mean_length = mean(ride_length_m)) %>% 
  arrange(member_casual, start_wday) %>% 
  ggplot(aes(x = start_wday, y = mean_length, 
             fill = member_casual)) + 
  geom_col(position = "dodge")
```





















This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
