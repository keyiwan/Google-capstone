---
title: "Google Data Analytics Certificate - Bike Share Case Study"
author: "K.Wan"
date: "2022/3/28"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale("LC_ALL",  "English") ## language set
```

## Case Background  

This analysis is for case study 1 from the Google Data Analytics Certificate (Cyclistic). It's originally based on the case study *[Sophisticated, Clear, and Polished: Divvy and Data Visualization](https://artscience.blog/home/divvy-dataviz-case-study)* by Kevin Hartman.  


I will be using the [Divvy dataset](https://divvy-tripdata.s3.amazonaws.com/index.html) for this capstone project. The purpose of this script is to consolidate downloaded Divvy data into a single dataframe and then conduct simple analysis to help answer the key question:"**In what ways do members and casual riders use Divvy bikes differently?**"  

In the Google Data Analytics Certificate programme, there are mainly six phases: **Ask**, **Prepare**, **Process**, **Analyse**, **Share**, and **Act**. Since this capstone challenge has outlined my tasks as a junior data analyst in this fictitious bike sharing company and provided real first-party data, my job will jump right into the **Process** of data. 

## Process

### Get data ready

For convenience, I will first load three useful packages: `tidyverse` for data import and wrangling, `lubridate` for date functions, and `ggplot2` for visualisation.

```{r loading packages, warning=FALSE}
# install.packages("tidyverse")
# install.packages("lubridate")

library(tidyverse) # data import and tidy
library(lubridate) # functions set for date, time
library(ggplot2) # helps visualise data
```

**1. Upload datasets**  

In this analysis, I will investigate data in the past twelve months, spanning from December 2021 to February 2022.

Function `readr::read_csv` is preferred to `read.csv` of package `base` for a number of reasons: <https://r4ds.had.co.nz/data-import.html#compared-to-base-r>.

```{r import data, message=FALSE, results='hide'}
dt_2202 <- read_csv("202202-divvy-tripdata.csv", na = "")
# na = "" is to assign blank cell with NA
dt_2201 <- read_csv("202201-divvy-tripdata.csv", na = "")
dt_2103 <- read_csv("202103-divvy-tripdata.csv", na = "")
dt_2104 <- read_csv("202104-divvy-tripdata.csv", na = "")
dt_2105 <- read_csv("202105-divvy-tripdata.csv", na = "")
dt_2106 <- read_csv("202106-divvy-tripdata.csv", na = "")
dt_2107 <- read_csv("202107-divvy-tripdata.csv", na = "")
dt_2108 <- read_csv("202108-divvy-tripdata.csv", na = "")
dt_2109 <- read_csv("202109-divvy-tripdata.csv", na = "")
dt_2110 <- read_csv("202110-divvy-tripdata.csv", na = "")
dt_2111 <- read_csv("202111-divvy-tripdata.csv", na = "")
dt_2112 <- read_csv("202112-divvy-tripdata.csv", na = "")
```

**2. Merging multiple tables**  

Before combining tables into a single data frame, we need to observe components of each table, specifically column names of each file, in order to make merge compatible.

Following code chunk displays some techniques we may use when dealing with this issue.

```{r merge method, eval=FALSE}
# check column names, for they need to match perfectly before we can combine files by rows
colnames(dt_2103)
colnames(dt_2104)
# if there are mismatched column names, we can rename columns to make them consisten, e.g.:
dt_2103 <- rename(dt_2103, new_name = old_name)
# inspect the dataframes and look for inconguencies
str(dt_2103)
# change column types if needed, e.g. convert ride_id to character so that they can stack correctly
dt_2103 <- mutate(dt_2103, ride_id = as.character(ride_id))
```

```{r merge}
dt_all <- rbind(dt_2103, dt_2104, dt_2105, dt_2106,
                dt_2107, dt_2108, dt_2109, dt_2110,
                dt_2111, dt_2112, dt_2201, dt_2202)
# dplyr::bind_rows() is a good alternative 
glimpse(dt_all)  # view the structure of the dataframe
old_records <- nrow(dt_all)
```

The resulting tibble consists of `r format(nrow(dt_all), big.mark = ",", scientific = FALSE)` rows with `r ncol(dt_all)` columns of character and numeric data such as ride id, start and end time of ride, start location, geographic coordinates, etc.


### Data cleaning

Data tidying usually involves checking for missing values, trailing white spaces, duplicate records, and other data anomalies.

**1. Duplicates**  

To check duplicates, `distinct()` is used to check duplicates based on all of columns while it is also an option to screen on `ride_id` alone. The result shows no duplicates removed.

```{r duplicates}
dt_all <- dt_all %>% 
  distinct()
# distinct(ride_id, .keep_all = TRUE), based on ride_id alone
## result in no duplicates removed
nrow_old <- nrow(dt_all)
```

**2. Missing values**  

```{r missing total, eval=FALSE, echo=FALSE}
apply(dt_all, 2, function (x) {sum(is.na(x))})
```

```{r missing proportion}
apply(dt_all, 2, function (x) {mean(is.na(x))})
```

Since there is no missing value on identifier column `ride_id` and the proportion of missing data on each variable is no more than 14%, it is practically acceptable to drop rows containing `NA`.

```{r drop_na}
dt_all <- dt_all %>% drop_na(start_station_name, start_station_id, end_station_name, end_station_id,
                       end_lat, end_lng)
nrow_new <- nrow(dt_all)
```

Then `r format(nrow_old - nrow_new, big.mark = ",", scientific = FALSE)` rows have been deleted, accounting for merely `r paste0((round((nrow_old-nrow_new)/nrow_old, digits=2))*100, "%")` of original data size.

**3. Strings**  

When it comes to characters, typos or miscellaneous errors are not uncommon. Error-prone strings can be due to careless data entry or even on purpose, they are not easy to spot and even more time-consuming or painstaking to fix.

Note that there are a few station ids representing more than one station name due to either typo or unknown minor changes. The following steps show how to identify mistyped station names and then conduct error checking.

However, manually fixing the problem of station names is not essential to answering questions of our interest and using station id alone is sufficient. 

```{r, eval=FALSE}
dup_id <- dt_all %>% 
  count(start_station_id, start_station_name) %>% 
  filter(duplicated(start_station_id)) %>% 
  select(start_station_id)

dup_id <- unlist(as.list(dup_id)) ## as.vector returns 
dt_all %>% 
  count(start_station_id, start_station_name) %>% 
  filter(start_station_id %in% dup_id)
```

**4. Dates**  

Addressing `date` columns is necessary for analysing the dynamics of rides on different time frames. With each ride record contains the exact time points for start and end, we can come up with a more meaningful metric - ride length - a calculated field using difference between columns `ended_at` and `started_at`.

```{r date formatting}
dt_all <- dt_all %>% 
  mutate(started_at = ymd_hms(started_at),
         ended_at = ymd_hms(ended_at),
         ride_length = ended_at - started_at) # figures in seconds, difftime is an alternative function
```

```{r drill-date, eval=FALSE, echo=FALSE}
dt_all %>% 
  mutate(ride_length = ended_at - started_at,
         start_day = mday(started_at),
         end_day = mday(ended_at),
         past_day = end_day - start_day) %>% 
  filter(past_day != 0) %>%  ## not on the same day, difftime as seconds, which accounts for day differences
  head(10)
```

Convert the unit of ride length from `seconds` to `minutes`.
```{r minutes}
dt_all <- dt_all %>% 
  mutate(ride_length_m = round((as.numeric(ride_length)/60), digits = 2))
```

Dissect `date` by year, month, day and weekday.  

The reason is that data can only be aggregated at the ride-level, which is too granular based on original date formats. So we will want to add some additional columns of data -- such as day, month, year, weekday -- that provide additional opportunities to aggregate the data.

```{r dissect date}
dt_all <- dt_all %>% 
  mutate(start_year = year(started_at), end_year = year(ended_at), start_month = month(started_at), end_month = month(ended_at), start_Month = month(started_at, label = TRUE), end_Month = month(ended_at, label = TRUE), start_day = mday(started_at), end_day = mday(ended_at), start_wday = wday(started_at, label = TRUE), end_wday = wday(ended_at, label = TRUE))
```

**5. Remove "bad" data**  

Despite no clear definition of "bad" data, here we need to spot data that takes only a small number while having values that can disproportionately influence our findings and at times delete them.

In this case, we consider getting rid of extremely large and negative values in `ride_length` since they make no practical sense for the majority of users and can distort data distribution, but how to address outliers is in fact a case-by-case puzzle.

Specifically, records for trips less than 60 seconds (false starts) or longer than 24 hours were removed. Bikes out longer than 24 hours are considered stolen and the rider is charged for a replacement. Idea inspired from: <https://vibrantoutlook.wordpress.com/2021/09/19/google-data-analytics-certificate-capstone-project/>

```{r filter, tidy=TRUE}
dt_all02 <- dt_all %>% 
  filter(end_day - start_day < 2 & 
           between(ride_length_m, 1, 1440))
## filter rides spanning across within 2 days while not lasting above 24 hours/ 1440 minutes, 1 for filtering out negative results and false starts
```

`r format(nrow_new - nrow(dt_all02), big.mark=",", scientific=FALSE)` records removed.

Meanwhile, rides that started or ended at DIVVY CASSETTE *REPAIR* MOBILE STATION or HUBBARD ST BIKE *CHECKING* (LBS-WH-TEST)  or WATSON *TESTING* DIVVY could be ignored as these are administrative stations. were removed as these are administrative stations. (Link for reference: <https://vibrantoutlook.wordpress.com/2021/09/19/google-data-analytics-certificate-capstone-project/>)  

For simplicity, I use `str_detect()` to find matches containing any characters of `REPAIR`, `TESTING` or `CHECKING`.

```{r testing, eval=FALSE, echo=FALSE}
## this code chunk is used for experimenting
station_name <- dt_all02 %>% 
  count(start_station_name)
station_name %>% 
  filter(str_detect(start_station_name, "REPAIR|TESTING|CHECKING")) 
# 4 records returned

ending_station_name <- dt_all02 %>% 
  count(end_station_name)

ending_station_name %>% 
  filter(str_detect(end_station_name, "REPAIR|TESTING|CHECKING")) # 4 records returned

station_name %>% 
  filter(start_station_name == "WATSON TESTING DIVVY") # double check for explicit search
```

```{r repair}
dt_all02 %>% 
  filter(str_detect(start_station_name, "REPAIR|TESTING|CHECKING") | str_detect(end_station_name, "REPAIR|TESTING|CHECKING")) %>% 
  select(ride_id, start_station_id, end_station_id)
```

8 records detected to be removed.

```{r nonrepair}
dt_all02 <- dt_all02 %>% 
  filter(!str_detect(start_station_name, "REPAIR|TESTING|CHECKING") & !str_detect(end_station_name, "REPAIR|TESTING|CHECKING")) # !(A | B) == !A & !B
```

At the end of data cleaning, `r format(old_records-nrow(dt_all02), big.mark = ",", scientific = FALSE)` rows have been deleted, accounting for `r paste0((round((old_records-nrow(dt_all02))/old_records, digits=2))*100, "%")` of original data size.

## Analyse

Here comes the first question: How many observations fall under each user type?

```{r category check}
(dt_all_freq <- dt_all02 %>% 
  count(member_casual))
```
As shown above, more than `r dt_all_freq[2,2]-dt_all_freq[1,2]` rides were by members than casual riders in the whole time period.

Then, we conduct descriptive analysis on ride length. Asking questions like:  

> Is there a difference on average ride lengths among the two user groups?  
> How is that difference varied on monthly/daily level?

**1. Total ride lengths**  

Let's have a look at the total, average, median, minimum and maximum.

```{r total length}
dt_all02 %>% 
  group_by(member_casual) %>% 
  summarise(number_of_rides = n(),
    total_length = sum(ride_length_m), mean_length = mean(ride_length_m),
    median_length = median(ride_length_m), max_length = max(ride_length_m), min_length = min(ride_length_m))
```
More rides taken by members while casual riders contribute to nearly double the size of the total length of members during the same time period.  

The average length of each ride by casual riders is  around `r dt_all02 %>% filter(member_casual=='casual') %>% summarise(mean_length_casual=mean(ride_length_m)) %>% round(digits=1)` minutes, more than double of the `r dt_all02 %>% filter(member_casual=='member') %>% summarise(mean_length_member=mean(ride_length_m)) %>% round(digits=1)` minutes for annual members.


**2. Ride length by month**

```{r month}
dt_all02 %>% 
  group_by(member_casual, start_Month) %>% 
  summarise(sum_length_m = sum(ride_length_m), mean_length_m = mean(ride_length_m)) %>% 
  arrange(member_casual, start_Month) %>% 
  head(12)
```

It seems like the second quarter (April, May & June) to be a period that experiences relatively longer duration of rides by casual riders.


**3. Which day of a week induces most traffic**

See the average ride time by each day for all users.
```{r weekday}
dt_all02 %>% 
  group_by(start_wday) %>% 
  summarise(number_of_rides = n(), total_length = sum(ride_length_m), mean_length = mean(ride_length_m))
```

Compare ridership, average tide time by weekday for members vs casual riders.

```{r weekday_type}
dt_all02 %>% 
  group_by(member_casual, start_wday) %>% 
  summarise(number_of_rides = n(), total_length = sum(ride_length_m), mean_length = mean(ride_length_m)) %>% 
  arrange(member_casual, start_wday) 
```

Regardless of the user type, weekends enjoy peak usage without doubt, which can potentially steer us in terms of resources allocation of advertising. 


**4. Top 10 busiest stations**

To better market membership of Divvy, hence attracting more casual riders to subscribe as members, stations mostly visited by casual users may be a great option as future campaign locations.

```{r hotspot01}
dt_all02 %>% 
  filter(member_casual == "casual") %>% 
  group_by(start_station_id) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n)) %>% 
  select(start_station_id, n) %>% 
  head(10)
```

Is there any difference in station preference between casual riders and annual members?

```{r hotspot02}
dt_all02 %>% 
  filter(member_casual == "member") %>% 
  group_by(start_station_id) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n)) %>% 
  select(start_station_id, n) %>% 
  head(10)
```

Overall, which stations can reach out to most users regardless of user type.

```{r hotspot03}
dt_all02 %>% 
  group_by(start_station_id) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n)) %>% 
  select(start_station_id, n) %>% 
  head(10)
```

As shown above, there is undeniably a distinct difference on stations choices made by members and casual riders.


**5. The proportion of casual riders and annual members for each station**

I want to produce a table with each row representing a unique station id, columns involving number of rides by members, by casual riders, and their individual proportion of total number of rides.

```{r proportion}
(prop_dt <- dt_all02 %>% 
  group_by(start_station_id, member_casual) %>% 
  summarise(n = n()))
```
This tibble having multiple values stacked in a small number of columns needs the function `pivot_wider` to make it wider.
```{r pivot_wider}
prop_dt %>% 
  pivot_wider(names_from = member_casual, values_from = n) %>% 
  mutate(total = casual + member, casual_freq = casual/total, member_freq = member/total) %>% 
  arrange(desc(casual))
```
In result, the list of 10 stations boasting far more visits from casual riders than members is the same as before.                    

## Share  

It is time to visualise our findings in order to share our report in a more vivid and engaging way.

**1. Members ride more often than non-members**  

Look at the stacked bar graph below, demonstrating the discrepancy on number of rides by rider type.

```{r rides_bar, echo=FALSE}
dt_all02 %>% 
  group_by(member_casual, start_wday) %>% 
  summarise(number_of_rides = n(), total_length = sum(ride_length_m), mean_length = mean(ride_length_m)) %>% 
  arrange(member_casual, start_wday) %>% 
  ggplot(aes(x = start_wday, y = number_of_rides, 
             fill = member_casual)) + 
  geom_col(position = "dodge")
```

```{r length_bar, echo=FALSE}
dt_all02 %>% 
  group_by(member_casual, start_wday) %>% 
  summarise(number_of_rides = n(), total_length = sum(ride_length_m), mean_length = mean(ride_length_m)) %>% 
  arrange(member_casual, start_wday) %>% 
  ggplot(aes(x = start_wday, y = mean_length, 
             fill = member_casual)) + 
  geom_col(position = "dodge")
```





















This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
